# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import whisper
import pyaudio
import wave
import os
import uuid
import warnings
from datetime import datetime
from pyannote.audio import Pipeline
import torch
import numpy as np
from pydub import AudioSegment
import logging
from dotenv import load_dotenv
from pathlib import Path

# .env íŒŒì¼ ê²½ë¡œ ì„¤ì •
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# HuggingFace í† í° ì„¤ì •
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    logger.error(f"HF_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (.env íŒŒì¼ ê²½ë¡œ: {env_path})")
    raise ValueError("HF_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Whisper ëª¨ë¸ ì´ˆê¸°í™”
model = whisper.load_model("large")
warnings.filterwarnings("ignore")

# Pyannote íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
try:
    logger.info("Pyannote íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=HF_TOKEN,
        cache_dir=os.path.expanduser("~/.cache/pyannote")
    )
    logger.info("Pyannote íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.error(f"Pyannote íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    logger.error("ë‹¤ìŒ ë§í¬ì—ì„œ ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì„ ìš”ì²­í•´ì£¼ì„¸ìš”:")
    logger.error("1. https://huggingface.co/pyannote/segmentation-3.0")
    logger.error("2. https://huggingface.co/pyannote/speaker-diarization-3.1")
    pipeline = None

# ì˜¤ë””ì˜¤ ë…¹ìŒ ì„¤ì •
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024
RECORD_SECONDS = 5
AUDIO_FOLDER = "data/recode/temp_audio" # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì €ì¥ë  í´ë” ê²½ë¡œ
OUTPUT_FOLDER = "data/recode"

# í´ë” ìƒì„±
os.makedirs(AUDIO_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def record_audio(filename):
    """ë§ˆì´í¬ë¡œë¶€í„° ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•˜ì—¬ WAV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                       rate=RATE, input=True,
                       frames_per_buffer=CHUNK)

    print("ğŸ¤ ìŒì„± ì…ë ¥ ì¤‘... ({}ì´ˆ)".format(RECORD_SECONDS))
    frames = []

    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)

    print("â¹ ë…¹ìŒ ì¢…ë£Œ")

    stream.stop_stream()
    stream.close()
    audio.terminate()

    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

def extract_speaker_audio(input_file, start_time, end_time, output_file):
    """íŠ¹ì • ì‹œê°„ ë²”ìœ„ì˜ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    audio = AudioSegment.from_wav(input_file)
    segment = audio[start_time*1000:end_time*1000]  # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    segment.export(output_file, format="wav")

def realtime_diarization():
    """ì‹¤ì‹œê°„ìœ¼ë¡œ ìŒì„±ì„ ë…¹ìŒí•˜ê³  í™”ì ë¶„ë¦¬ ë° í…ìŠ¤íŠ¸ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    if pipeline is None:
        logger.error("Pyannote íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print("ğŸš€ ì‹¤ì‹œê°„ í™”ì ë¶„ë¦¬ ë° í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C)")

    try:
        while True:
            # ì„ì‹œ WAV íŒŒì¼ ìƒì„±
            temp_file = os.path.join(AUDIO_FOLDER, f"{uuid.uuid4()}.wav")
            record_audio(temp_file)

            print("ğŸ§  í™”ì ë¶„ë¦¬ ì¤‘...")
            try:
                # í™”ì ë¶„ë¦¬ ìˆ˜í–‰
                diarization = pipeline(temp_file)
                
                # ê²°ê³¼ë¥¼ ì €ì¥í•  í…ìŠ¤íŠ¸
                result_text = []
                
                # ê° í™”ìë³„ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ë³€í™˜
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    start_time = turn.start
                    end_time = turn.end
                    
                    # í™”ìë³„ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                    speaker_audio = os.path.join(AUDIO_FOLDER, f"{uuid.uuid4()}_{speaker}.wav")
                    extract_speaker_audio(temp_file, start_time, end_time, speaker_audio)
                    
                    # Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
                    result = model.transcribe(speaker_audio, language="ko")
                    result_text.append(f"{speaker}: {result['text']}")
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    os.remove(speaker_audio)

                # ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
                current_time = datetime.now().strftime("%Y%m%d_%H")
                output_file = os.path.join(OUTPUT_FOLDER, f"{current_time}.txt")
                
                with open(output_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(result_text) + "\n\n")
                
                print("ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:")
                for line in result_text:
                    print(line)
                
            except Exception as e:
                logger.error(f"í™”ì ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                print("í™”ì ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë…¹ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_file):
                    os.remove(temp_file)

    except KeyboardInterrupt:
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    realtime_diarization()
